{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>date_published</th>\n",
       "      <th>explanation</th>\n",
       "      <th>fact_checkers</th>\n",
       "      <th>main_text</th>\n",
       "      <th>sources</th>\n",
       "      <th>label</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15661</td>\n",
       "      <td>\"The money the Clinton Foundation took from fr...</td>\n",
       "      <td>April 26, 2015</td>\n",
       "      <td>\"Gingrich said the Clinton Foundation \"\"took m...</td>\n",
       "      <td>Katie Sanders</td>\n",
       "      <td>\"Hillary Clinton is in the political crosshair...</td>\n",
       "      <td>https://www.wsj.com/articles/clinton-foundatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>Foreign Policy, PunditFact, Newt Gingrich,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9893</td>\n",
       "      <td>Annual Mammograms May Have More False-Positives</td>\n",
       "      <td>October 18, 2011</td>\n",
       "      <td>This article reports on the results of a study...</td>\n",
       "      <td></td>\n",
       "      <td>While the financial costs of screening mammogr...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Screening,WebMD,women's health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11358</td>\n",
       "      <td>SBRT Offers Prostate Cancer Patients High Canc...</td>\n",
       "      <td>September 28, 2016</td>\n",
       "      <td>This news release describes five-year outcomes...</td>\n",
       "      <td>Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...</td>\n",
       "      <td>The news release quotes lead researcher Robert...</td>\n",
       "      <td>https://www.healthnewsreview.org/wp-content/up...</td>\n",
       "      <td>1</td>\n",
       "      <td>Association/Society news release,Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10166</td>\n",
       "      <td>Study: Vaccine for Breast, Ovarian Cancer Has ...</td>\n",
       "      <td>November 8, 2011</td>\n",
       "      <td>While the story does many things well, the ove...</td>\n",
       "      <td></td>\n",
       "      <td>The story does discuss costs, but the framing ...</td>\n",
       "      <td>http://clinicaltrials.gov/ct2/results?term=can...</td>\n",
       "      <td>2</td>\n",
       "      <td>Cancer,WebMD,women's health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11276</td>\n",
       "      <td>Some appendicitis cases may not require ’emerg...</td>\n",
       "      <td>September 20, 2010</td>\n",
       "      <td>We really don’t understand why only a handful ...</td>\n",
       "      <td></td>\n",
       "      <td>\"Although the story didn’t cite the cost of ap...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_id                                              claim  \\\n",
       "0    15661  \"The money the Clinton Foundation took from fr...   \n",
       "1     9893    Annual Mammograms May Have More False-Positives   \n",
       "2    11358  SBRT Offers Prostate Cancer Patients High Canc...   \n",
       "3    10166  Study: Vaccine for Breast, Ovarian Cancer Has ...   \n",
       "4    11276  Some appendicitis cases may not require ’emerg...   \n",
       "\n",
       "       date_published                                        explanation  \\\n",
       "0      April 26, 2015  \"Gingrich said the Clinton Foundation \"\"took m...   \n",
       "1    October 18, 2011  This article reports on the results of a study...   \n",
       "2  September 28, 2016  This news release describes five-year outcomes...   \n",
       "3    November 8, 2011  While the story does many things well, the ove...   \n",
       "4  September 20, 2010  We really don’t understand why only a handful ...   \n",
       "\n",
       "                                       fact_checkers  \\\n",
       "0                                      Katie Sanders   \n",
       "1                                                      \n",
       "2  Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                           main_text  \\\n",
       "0  \"Hillary Clinton is in the political crosshair...   \n",
       "1  While the financial costs of screening mammogr...   \n",
       "2  The news release quotes lead researcher Robert...   \n",
       "3  The story does discuss costs, but the framing ...   \n",
       "4  \"Although the story didn’t cite the cost of ap...   \n",
       "\n",
       "                                             sources  label  \\\n",
       "0  https://www.wsj.com/articles/clinton-foundatio...      0   \n",
       "1                                                         1   \n",
       "2  https://www.healthnewsreview.org/wp-content/up...      1   \n",
       "3  http://clinicaltrials.gov/ct2/results?term=can...      2   \n",
       "4                                                         2   \n",
       "\n",
       "                                      subjects  \n",
       "0  Foreign Policy, PunditFact, Newt Gingrich,   \n",
       "1               Screening,WebMD,women's health  \n",
       "2      Association/Society news release,Cancer  \n",
       "3                  Cancer,WebMD,women's health  \n",
       "4                                               "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw dataset from ./data/raw...\n",
      "Initializing tokenizer for nbroad/bigbird-base-health-fact...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9832/9832 [00:01<00:00, 7898.11 examples/s]\n",
      "Map: 100%|██████████| 1225/1225 [00:00<00:00, 7738.43 examples/s]\n",
      "Map: 100%|██████████| 1235/1235 [00:00<00:00, 8040.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding labels to tokenized dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9832/9832 [00:00<00:00, 276552.46 examples/s]\n",
      "Map: 100%|██████████| 1225/1225 [00:00<00:00, 134232.63 examples/s]\n",
      "Map: 100%|██████████| 1235/1235 [00:00<00:00, 197603.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting dataset for PyTorch...\n",
      "Saving processed features to ./data/features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 9832/9832 [00:00<00:00, 408309.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1225/1225 [00:00<00:00, 244714.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1235/1235 [00:00<00:00, 269173.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved successfully in ./data/features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def preprocess_dataset(input_dir: str, output_dir: str, model_name: str, max_length: int) -> None:\n",
    "\n",
    "    print(f\"Loading raw dataset from {input_dir}.\")\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": load_dataset(\"csv\", data_files=os.path.join(input_dir, \"train.csv\"))[\"train\"],\n",
    "        \"validation\": load_dataset(\"csv\", data_files=os.path.join(input_dir, \"validation.csv\"))[\"train\"],\n",
    "        \"test\": load_dataset(\"csv\", data_files=os.path.join(input_dir, \"test.csv\"))[\"train\"]\n",
    "    })\n",
    "\n",
    "    print(f\"Initializing tokenizer for {model_name}.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = [\n",
    "            (claim if claim else \"\") + \" \" + (explanation if explanation else \"\")\n",
    "            for claim, explanation in zip(examples[\"claim\"], examples[\"explanation\"])\n",
    "        ]\n",
    "        return tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "\n",
    "    print(\"Tokenizing dataset\")\n",
    "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    print(\"Adding labels to tokenized dataset\")\n",
    "    def add_labels(examples):\n",
    "        return {\"labels\": examples[\"label\"]}\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "\n",
    "    print(\"Formatting dataset\")\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving processed features to {output_dir}\")\n",
    "    tokenized_dataset.save_to_disk(output_dir)\n",
    "    print(f\"Features saved successfully in {output_dir}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_DIR = \"./data/raw\"\n",
    "    OUTPUT_DIR = \"./data/features\"\n",
    "    MODEL_NAME = \"nbroad/bigbird-base-health-fact\"\n",
    "    MAX_LENGTH = 512\n",
    "\n",
    "    preprocess_dataset(INPUT_DIR, OUTPUT_DIR, MODEL_NAME, MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw dataset from ./../data/raw...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m INPUT_DIR\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading raw dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m })\n",
      "File \u001b[0;32m/opt/miniconda3/envs/quantum1/lib/python3.12/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "INPUT_DIR = \"./../data/raw\"\n",
    "input_dir = INPUT_DIR\n",
    "print(f\"Loading raw dataset from {input_dir}...\")\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"csv\", data_files=os.path.join(input_dir, \"train.csv\"))[\"train\"],\n",
    "    \"validation\": load_dataset(\"csv\", data_files=os.path.join(input_dir, \"validation.csv\"))[\"validation\"],\n",
    "    \"test\": load_dataset(\"csv\", data_files=os.path.join(input_dir, \"test.csv\"))[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim_id', 'claim', 'date_published', 'explanation', 'fact_checkers', 'main_text', 'sources', 'label', 'subjects'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['claim_id', 'claim', 'date_published', 'explanation', 'fact_checkers', 'main_text', 'sources', 'label', 'subjects'],\n",
       "        num_rows: 1225\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim_id', 'claim', 'date_published', 'explanation', 'fact_checkers', 'main_text', 'sources', 'label', 'subjects'],\n",
       "        num_rows: 1235\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
